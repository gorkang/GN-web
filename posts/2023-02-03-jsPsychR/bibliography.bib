@article{deleeuw2015br,
  ids = {deleeuw2015brm},
  title = {{{jsPsych}}: {{A JavaScript}} Library for Creating Behavioral Experiments in a {{Web}} Browser},
  shorttitle = {{{jsPsych}}},
  author = {family=Leeuw, given=Joshua R., prefix=de, useprefix=true},
  date = {2015-03},
  journaltitle = {Behavior Research Methods},
  shortjournal = {Behav Res},
  volume = {47},
  number = {1},
  pages = {1--12},
  issn = {1554-3528},
  doi = {10.3758/s13428-014-0458-y},
  url = {http://link.springer.com/10.3758/s13428-014-0458-y},
  urldate = {2021-10-16},
  abstract = {Online experiments are growing in popularity, and the increasing sophistication of Web technology has made it possible to run complex behavioral experiments online using only a Web browser. Unlike with offline laboratory experiments, however, few tools exist to aid in the development of browser-based experiments. This makes the process of creating an experiment slow and challenging, particularly for researchers who lack a Web development background. This article introduces jsPsych, a JavaScript library for the development of Web-based experiments. jsPsych formalizes a way of describing experiments that is much simpler than writing the entire experiment from scratch. jsPsych then executes these descriptions automatically, handling the flow from one task to another. The jsPsych library is opensource and designed to be expanded by the research community. The project is available online at www.jspsych.org.},
  langid = {english},
  annotation = {ZSCC: 0000589},
  file = {/home/emrys/gorkang@gmail.com/RESEARCH/zotero-library/AUTH/de Leeuw/de Leeuw_2015_jsPsych.pdf}
}

@article{knuth84,
  author = {Knuth, Donald E.},
  title = {Literate Programming},
  year = {1984},
  issue_date = {May 1984},
  publisher = {Oxford University Press, Inc.},
  address = {USA},
  volume = {27},
  number = {2},
  issn = {0010-4620},
  url = {https://doi.org/10.1093/comjnl/27.2.97},
  doi = {10.1093/comjnl/27.2.97},
  journal = {Comput. J.},
  month = may,
  pages = {97–111},
  numpages = {15}
}



@article{lindsay2023m,
  title = {A {{Plea}} to {{Psychology Professional Societies}} That {{Publish Journals}}: {{Assess Computational Reproducibility}}},
  shorttitle = {A {{Plea}} to {{Psychology Professional Societies}} That {{Publish Journals}}},
  author = {Lindsay, D. Stephen},
  date = {2023-09-27},
  journaltitle = {Meta-Psychology},
  shortjournal = {MP},
  volume = {7},
  issn = {2003-2714},
  doi = {10.15626/MP.2023.4020},
  url = {https://open.lnu.se/index.php/metapsychology/article/view/4020},
  urldate = {2023-10-02},
  langid = {english},
  file = {/home/emrys/Zotero/storage/TSE73NND/Lindsay - 2023 - A Plea to Psychology Professional Societies that P.pdf}
}

@article{obels2020ampps,
  title = {Analysis of {{Open Data}} and {{Computational Reproducibility}} in {{Registered Reports}} in {{Psychology}}},
  author = {Obels, Pepijn and Lakens, Daniël and Coles, Nicholas A and Gottfried, Jaroslav and Green, Seth A},
  date = {2020},
  journaltitle = {Advances in Methods and Practices in Psychological Science},
  volume = {3},
  number = {2},
  doi = {10.1177/2515245920918872},
  abstract = {Ongoing technological developments have made it easier than ever before for scientists to share their data, materials, and analysis code. Sharing data and analysis code makes it easier for other researchers to reuse or check published research. However, these benefits will emerge only if researchers can reproduce the analyses reported in published articles and if data are annotated well enough so that it is clear what all variable and value labels mean. Because most researchers are not trained in computational reproducibility, it is important to evaluate current practices to identify those that can be improved. We examined data and code sharing for Registered Reports published in the psychological literature from 2014 to 2018 and attempted to independently computationally reproduce the main results in each article. Of the 62 articles that met our inclusion criteria, 41 had data available, and 37 had analysis scripts available. Both data and code for 36 of the articles were shared. We could run the scripts for 31 analyses, and we reproduced the main results for 21 articles. Although the percentage of articles for which both data and code were shared (36 out of 62, or 58\%) and the percentage of articles for which main results could be computationally reproduced (21 out of 36, or 58\%) were relatively high compared with the percentages found in other studies, there is clear room for improvement. We provide practical recommendations based on our observations and cite examples of good research practices in the studies whose main results we reproduced.},
  langid = {english},
  keywords = {❓ Multiple DOI},
  file = {/home/emrys/Zotero/storage/ZY2E2Z6Z/Obels et al. - Analysis of Open Data and Computational Reproducib.pdf}
}

@article{targets,
	title = {The targets R package: a dynamic Make-like function-oriented pipeline toolkit for reproducibility and high-performance computing},
	author = {Landau, William Michael},
	year = {2021},
	date = {2021},
	pages = {2959},
	volume = {6},
	url = {https://doi.org/10.21105/joss.02959}
}

